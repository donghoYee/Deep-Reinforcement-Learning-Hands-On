{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a90aec7-0d6b-4485-982d-d51e0f37f60b",
   "metadata": {},
   "source": [
    "# The Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ff1ef-94cd-48ee-b4a5-4d40e1ffe175",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. On CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7554c540-6fa3-42f8-a622-182627c723fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter # to log values\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e558d0d3-d14d-4733-8b13-4de16b8ceb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparams\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16 # number of episodes to play on every batch\n",
    "PERCENTILE = 70 # to limit good-result ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3caa266-2792-4ca8-b60a-aba81299b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d204d74b-ddd6-46fa-a135-00ea276eac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple(\"Episode\", field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=\n",
    "                         ['observation', 'action'])\n",
    "\n",
    "# two classes to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a20d036-e579-47d4-b009-9c6c60247434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size): # generate batch of data\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs= env.reset()\n",
    "    sm = nn.Softmax(dim=1) # softmax function\n",
    "    \n",
    "    while True: # infinite dataset\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v)) # map obs to action\n",
    "        act_probs = act_probs_v.data.numpy()[0] # only one env at a time\n",
    "        \n",
    "        action= np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action) # step env randomly\n",
    "        \n",
    "        episode_reward += reward # gamma is 1\n",
    "        step = EpisodeStep(observation=obs, action=action) # tuple, but named\n",
    "        episode_steps.append(step)\n",
    "        \n",
    "        if is_done:\n",
    "            e= Episode(reward= episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            \n",
    "            episode_reward= 0\n",
    "            episode_steps= []\n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch # return it just now\n",
    "                batch = []\n",
    "                \n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6cf945-1681-42ba-970f-d881b7bf4edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list( map(lambda s: s.reward, batch)) # episode.reward for every episode in batch\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards)) # only for monitor\n",
    "    \n",
    "    # filter our episodes\n",
    "    train_obs = []\n",
    "    train_act= []\n",
    "    for reward, steps in batch: # batch is filled with tuple\n",
    "        if reward < reward_bound:\n",
    "            continue # to simplify coding\n",
    "        \n",
    "        train_obs.extend([step.observation for step in steps])\n",
    "        train_act.extend([step.action for step in steps])\n",
    "        # not seperated by episodes\n",
    "        \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    \n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79c63e1-5b66-41fc-b226-512d8c757267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay, os\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "# to enable recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d1a142a-89f4-4c82-926e-b8ad73605b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.507, reward_mean=196.1, reward_boud=200.0\n",
      "1: loss=0.504, reward_mean=149.0, reward_boud=154.5\n",
      "2: loss=0.495, reward_mean=173.3, reward_boud=200.0\n",
      "3: loss=0.499, reward_mean=196.1, reward_boud=200.0\n",
      "4: loss=0.492, reward_mean=200.0, reward_boud=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env,directory=\"cartpole_mon\", force=True)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss() # instantize!!\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer= SummaryWriter(comment=\"-cartpole\") # add comment\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(\n",
    "    env, net, BATCH_SIZE)): # infinite dataset using yeild\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(\n",
    "        batch, PERCENTILE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward() # backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_boud=%.1f\" %(\n",
    "          iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    \n",
    "    if reward_m > 199.0:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a2c4e-272b-4f40-9eeb-2ab91606877b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DRLH]",
   "language": "python",
   "name": "conda-env-DRLH-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
